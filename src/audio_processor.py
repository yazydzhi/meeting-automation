#!/usr/bin/env python3
"""
Audio Processor with Whisper Integration
–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º OpenAI Whisper API
–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
"""

import os
import sys
import json
import tempfile
import subprocess
import shutil
from datetime import datetime
from typing import List, Dict, Any, Optional
import logging
from logging.handlers import RotatingFileHandler
import time # –î–æ–±–∞–≤–ª—è–µ–º –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π
sys.path.append(str(Path(__file__).parent.parent))

try:
    from pydub import AudioSegment
    from pydub.silence import split_on_silence
    import librosa
    import numpy as np
    from scipy.signal import find_peaks
    import noisereduce as nr
except ImportError as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install pydub librosa numpy scipy noisereduce")
    sys.exit(1)

from src.config_manager import ConfigManager

class AudioProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª–æ–≤ —Å Whisper –∏ —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏"""
    
    def __init__(self, config_file: str = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AudioProcessor"""
        # –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        self.config = self._load_config(config_file)
        
        # –ó–∞—Ç–µ–º –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.logger = self._setup_logging()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ—Ä –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–∫
        self.transcription_method = self.config.get('TRANSCRIPTION_METHOD', 'openai')
        
        self.logger.info(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AudioProcessor —Å –º–µ—Ç–æ–¥–æ–º: {self.transcription_method}")
        self.logger.info(f"üîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {self.config}")
        
        if self.transcription_method == 'openai':
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ API –∫–ª—é—á–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è OpenAI
            if not self.config.get('OPENAI_API_KEY'):
                raise ValueError("OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è OpenAI API")
            self.logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é OpenAI API...")
            self.client = self._setup_openai()
            self.logger.info(f"üîß OpenAI –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {self.client is not None}")
        elif self.transcription_method == 'whisper':
            self.logger.info("üé§ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é –ª–æ–∫–∞–ª—å–Ω—ã–π Whisper...")
            self.whisper_model = self._setup_openai_whisper()
            self.logger.info(f"üîß Whisper –º–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: {self.whisper_model is not None}")
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {self.transcription_method}. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ: openai, whisper")
            
        # –í –∫–æ–Ω—Ü–µ —Å–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self._setup_directories()
        
    def _load_config(self, config_file: str = None) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            if config_file and os.path.exists(config_file):
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
                config_manager = ConfigManager(config_file)
                config = config_manager.config
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Whisper –∏–∑ –Ω–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                whisper_config = config.get('whisper', {})
                return {
                    'TRANSCRIPTION_METHOD': whisper_config.get('transcription_method', 'openai'),
                    'OPENAI_API_KEY': whisper_config.get('openai_api_key', ''),
                    'WHISPER_MODEL': whisper_config.get('whisper_model', 'whisper-1'),
                    'WHISPER_BEAM_SIZE': int(whisper_config.get('whisper_beam_size', '5')),
                    'WHISPER_TEMPERATURE': float(whisper_config.get('whisper_temperature', '0.0')),
                    'WHISPER_MODEL_LOCAL': whisper_config.get('whisper_model_local', 'base'),
                    'WHISPER_LANGUAGE': whisper_config.get('whisper_language', 'ru'),
                    'WHISPER_TASK': whisper_config.get('whisper_task', 'transcribe'),
                    'REMOVE_ECHO': whisper_config.get('remove_echo', True),
                    'AUDIO_NORMALIZE': whisper_config.get('audio_normalize', True),
                    'TEMP_AUDIO_ROOT': whisper_config.get('temp_audio_root', 'data/temp_audio'),
                    'TRANSCRIPT_OUTPUT_ROOT': 'data/transcripts',
                    'AUDIO_PROCESSING_ROOT': 'data/audio_processing',
                    'LOG_FILE': 'logs/audio_processing.log'
                }
            else:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
                config = {}
                
                # OpenAI –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
                config['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')
                config['WHISPER_MODEL'] = os.getenv('WHISPER_MODEL', 'whisper-1')
                config['WHISPER_BEAM_SIZE'] = int(os.getenv('WHISPER_BEAM_SIZE', '5'))
                config['WHISPER_TEMPERATURE'] = float(os.getenv('WHISPER_TEMPERATURE', '0.0'))
                
                # –ú–µ—Ç–æ–¥ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
                config['TRANSCRIPTION_METHOD'] = os.getenv('TRANSCRIPTION_METHOD', 'openai')
                
                # –õ–æ–∫–∞–ª—å–Ω—ã–π Whisper –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
                config['WHISPER_MODEL_LOCAL'] = os.getenv('WHISPER_MODEL_LOCAL', 'base')
                
                return config
                
        except Exception as e:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            return {
                'TRANSCRIPTION_METHOD': 'openai',
                'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY'),  # –î–æ–±–∞–≤–ª—è–µ–º API –∫–ª—é—á
                'WHISPER_MODEL': 'whisper-1',
                'WHISPER_MODEL_LOCAL': 'base',
                'TEMP_AUDIO_ROOT': 'data/temp_audio',
                'TRANSCRIPT_OUTPUT_ROOT': 'data/transcripts',
                'AUDIO_PROCESSING_ROOT': 'data/audio_processing',
                'LOG_FILE': 'logs/audio_processing.log'
            }
            
    def _setup_faster_whisper(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ faster-whisper –º–æ–¥–µ–ª–∏"""
        try:
            from faster_whisper import WhisperModel
            
            model_size = self.config.get('FASTER_WHISPER_MODEL', 'base')
            device = self.config.get('FASTER_WHISPER_DEVICE', 'cpu')
            compute_type = self.config.get('FASTER_WHISPER_COMPUTE_TYPE', 'float32')
            
            self.logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é faster-whisper –º–æ–¥–µ–ª—å: {model_size} –Ω–∞ {device}")
            
            model = WhisperModel(
                model_size_or_path=model_size,
                device=device,
                compute_type=compute_type,
                download_root=None,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                local_files_only=False
            )
            
            self.logger.info(f"faster-whisper –º–æ–¥–µ–ª—å {model_size} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return model
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ faster-whisper: {e}")
            raise
            
    def _setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        log_level = self.config.get('LOG_LEVEL', 'INFO')
        log_file = self.config.get('LOG_FILE', 'logs/audio_processing.log')
        
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
        
        logging.basicConfig(
            level=getattr(logging, log_level.upper()),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                RotatingFileHandler(
                    log_file,
                    maxBytes=int(os.getenv("LOG_MAX_SIZE_MB", "100")) * 1024 * 1024,
                    backupCount=int(os.getenv("LOG_BACKUP_COUNT", "5")),
                    encoding="utf-8"
                ),
                logging.StreamHandler()
            ]
        )
        
        logger = logging.getLogger(__name__)
        return logger
            
    def _setup_openai(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ OpenAI API"""
        api_key = self.config.get('OPENAI_API_KEY')
        self.logger.info(f"üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ OpenAI API —Å –∫–ª—é—á–æ–º: {api_key[:10]}...")
        
        if not api_key:
            raise ValueError("OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            
        try:
            from openai import OpenAI
            self.logger.info("üîß –ò–º–ø–æ—Ä—Ç OpenAI –º–æ–¥—É–ª—è —É—Å–ø–µ—à–µ–Ω")
            self.client = OpenAI(api_key=api_key)
            self.logger.info(f"üîß OpenAI –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω: {self.client}")
            self.logger.info("OpenAI API –Ω–∞—Å—Ç—Ä–æ–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            return self.client
        except ImportError as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ OpenAI: {e}")
            raise ImportError("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ openai: pip install openai")
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è OpenAI –∫–ª–∏–µ–Ω—Ç–∞: {e}")
            raise
            
    def _setup_openai_whisper(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ OpenAI Whisper"""
        try:
            import whisper
            
            model_size = self.config.get('WHISPER_MODEL_LOCAL', 'base')
            self.logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é OpenAI Whisper –º–æ–¥–µ–ª—å: {model_size}")
            
            model = whisper.load_model(model_size)
            
            self.logger.info(f"OpenAI Whisper –º–æ–¥–µ–ª—å {model_size} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return model
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ OpenAI Whisper: {e}")
            raise
            
    def _setup_directories(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
        dirs = [
            self.config.get('TEMP_AUDIO_ROOT', 'data/temp_audio'),
            self.config.get('TRANSCRIPT_OUTPUT_ROOT', 'data/transcripts'),
            self.config.get('AUDIO_PROCESSING_ROOT', 'data/audio_processing')
        ]
        
        for dir_path in dirs:
            os.makedirs(dir_path, exist_ok=True)
            
    def process_audio_file(self, audio_file_path: str, output_format: str = 'json') -> Dict[str, Any]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞"""
        self.logger.info(f"–ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–∞: {audio_file_path}")
        
        if not os.path.exists(audio_file_path):
            raise FileNotFoundError(f"–ê—É–¥–∏–æ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_file_path}")
            
        # –®–∞–≥ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞—É–¥–∏–æ (–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —ç—Ö–∞)
        prepared_audio_path = self._prepare_audio(audio_file_path)
        
        # –®–∞–≥ 2: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã –ø–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º –≥–æ–ª–æ—Å–∞
        segments = self._split_audio_by_voice_characteristics(prepared_audio_path)
        self.logger.info(f"–ê—É–¥–∏–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–æ –Ω–∞ {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
        
        # –®–∞–≥ 3: –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
        transcriptions = []
        for i, segment in enumerate(segments):
            self.logger.info(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é —Å–µ–≥–º–µ–Ω—Ç {i+1}/{len(segments)}")
            transcript = self._transcribe_with_whisper(segment, prepared_audio_path)
            if transcript:
                transcriptions.append({
                    'segment': i + 1,
                    'start_time': segment['start_time'],
                    'end_time': segment['end_time'],
                    'text': transcript,
                    'duration': segment['duration']
                })
                
        # –®–∞–≥ 4: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π
        speaker_groups = self._group_by_speakers_improved(transcriptions)
        
        # –®–∞–≥ 5: –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        result = {
            'file_path': audio_file_path,
            'file_size': os.path.getsize(audio_file_path),
            'processed_at': datetime.now().isoformat(),
            'total_segments': len(segments),
            'total_duration': sum(seg['duration'] for seg in segments),
            'speakers': speaker_groups,
            'raw_transcriptions': transcriptions,
            'processing_info': {
                'echo_removed': self.config.get('REMOVE_ECHO', True),
                'audio_normalized': self.config.get('AUDIO_NORMALIZE', True),
                'voice_analysis_method': self.config.get('VOICE_ANALYSIS_METHOD', 'mfcc'),
                'whisper_model': self.config.get('WHISPER_MODEL', 'whisper-1'),
                'original_audio_file': audio_file_path,
                'prepared_audio_file': prepared_audio_path
            }
        }
        
        self.logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç: {len(speaker_groups)} —Å–ø–∏–∫–µ—Ä–æ–≤")
        self.logger.info(f"–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {list(result.keys())}")
        self.logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π: {len(transcriptions)}")
        self.logger.info(f"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {result['file_size']} –±–∞–π—Ç")
        self.logger.info(f"–û–±—â–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {result['total_duration']} –º—Å")
        
        # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        # self.cleanup_temp_files()  # –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        
        return result
        
    def process_audio_file_full(self, audio_file_path: str, output_format: str = 'json') -> Dict[str, Any]:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞ –±–µ–∑ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        self.logger.info(f"–ù–∞—á–∏–Ω–∞—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –ø–æ–ª–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {audio_file_path}")
        
        if not os.path.exists(audio_file_path):
            raise FileNotFoundError(f"–ê—É–¥–∏–æ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_file_path}")
            
        # –®–∞–≥ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞—É–¥–∏–æ (–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —ç—Ö–∞)
        prepared_audio_path = self._prepare_audio(audio_file_path)
        
        # –®–∞–≥ 2: –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        self.logger.info("–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é –ø–æ–ª–Ω—ã–π –∞—É–¥–∏–æ —Ñ–∞–π–ª...")
        full_transcript = self._transcribe_full_audio(prepared_audio_path)
        
        if not full_transcript:
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –ø–æ–ª–Ω–æ–≥–æ —Ñ–∞–π–ª–∞")
            
        # –®–∞–≥ 3: –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        result = {
            'file_path': audio_file_path,
            'file_size': os.path.getsize(audio_file_path),
            'processed_at': datetime.now().isoformat(),
            'total_segments': 1,
            'total_duration': self._get_audio_duration(prepared_audio_path),
            'speakers': {
                '–£—á–∞—Å—Ç–Ω–∏–∫ 1': [{
                    'segment': 1,
                    'start_time': 0,
                    'end_time': self._get_audio_duration(prepared_audio_path),
                    'text': full_transcript,
                    'duration': self._get_audio_duration(prepared_audio_path)
                }]
            },
            'raw_transcriptions': [{
                'segment': 1,
                'start_time': 0,
                'end_time': self._get_audio_duration(prepared_audio_path),
                'text': full_transcript,
                'duration': self._get_audio_duration(prepared_audio_path)
            }],
            'processing_info': {
                'echo_removed': self.config.get('REMOVE_ECHO', True),
                'audio_normalized': self.config.get('AUDIO_NORMALIZE', True),
                'transcription_method': 'full_file',
                'whisper_model': self.config.get('WHISPER_MODEL', 'whisper-1'),
                'original_audio_file': audio_file_path,
                'prepared_audio_file': prepared_audio_path
            }
        }
        
        self.logger.info(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {result['file_size']} –±–∞–π—Ç")
        self.logger.info(f"–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ: {result['total_duration']} –º—Å")
        self.logger.info(f"–î–ª–∏–Ω–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞: {len(full_transcript)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        return result
        
    def process_audio_file_with_advanced_segmentation(self, audio_file_path: str, segmentation_method: str = 'adaptive') -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        try:
            self.logger.info(f"–ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π: {segmentation_method}")
            
            if not os.path.exists(audio_file_path):
                raise FileNotFoundError(f"–ê—É–¥–∏–æ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_file_path}")
            
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥—É–ª—å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            try:
                from advanced_segmentation import AdvancedSegmentation
            except ImportError:
                self.logger.error("–ú–æ–¥—É–ª—å advanced_segmentation –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return self.process_audio_file_full(audio_file_path)
            
            # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä–∞
            segmenter = AdvancedSegmentation(self.config)
            
            # –®–∞–≥ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞—É–¥–∏–æ
            prepared_audio_path = self._prepare_audio(audio_file_path)
            
            # –®–∞–≥ 2: –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–µ—Ç–æ–¥–∞
            if segmentation_method == 'intonation':
                segments = segmenter.segment_by_intonation_patterns(prepared_audio_path)
            elif segmentation_method == 'emotion':
                segments = segmenter.segment_by_emotional_patterns(prepared_audio_path)
            elif segmentation_method == 'energy':
                segments = segmenter.segment_by_energy_patterns(prepared_audio_path)
            elif segmentation_method == 'adaptive':
                segments = segmenter.segment_by_context_awareness(prepared_audio_path)
            else:
                self.logger.warning(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {segmentation_method}, –∏—Å–ø–æ–ª—å–∑—É—é –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π")
                segments = segmenter.segment_by_context_awareness(prepared_audio_path)
            
            if not segments:
                self.logger.warning("–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–µ –¥–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é –ø–æ–ª–Ω—É—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é")
                return self.process_audio_file_full(audio_file_path)
            
            # –®–∞–≥ 3: –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
            transcriptions = []
            for i, segment in enumerate(segments):
                self.logger.info(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é —Å–µ–≥–º–µ–Ω—Ç {i+1}/{len(segments)} ({segment.get('segmentation_method', 'unknown')})")
                
                # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞
                segment_audio = segment.get('audio')
                if segment_audio is not None:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–µ–≥–º–µ–Ω—Ç –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                    temp_segment_path = os.path.join(
                        self.config.get('TEMP_AUDIO_ROOT', 'data/temp_audio'),
                        f"segment_{i+1:03d}_{segment['start_time']:06d}ms_{segment['end_time']:06d}ms.wav"
                    )
                    
                    import soundfile as sf
                    sf.write(temp_segment_path, segment_audio, 16000)
                    
                    # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º —Å–µ–≥–º–µ–Ω—Ç
                    transcript = self._transcribe_full_audio(temp_segment_path)
                    
                    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–µ–≥–º–µ–Ω—Ç–∞
                    try:
                        os.remove(temp_segment_path)
                    except:
                        pass
                    
                    if transcript:
                        transcriptions.append({
                            'segment': i + 1,
                            'start_time': segment['start_time'],
                            'end_time': segment['end_time'],
                            'duration': segment['duration'],
                            'text': transcript,
                            'segmentation_method': segment.get('segmentation_method', 'unknown')
                        })
                else:
                    self.logger.warning(f"–°–µ–≥–º–µ–Ω—Ç {i+1} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã—Ö")
            
            # –®–∞–≥ 4: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º
            speaker_groups = self._group_by_speakers_improved(transcriptions)
            
            # –®–∞–≥ 5: –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            result = {
                'file_path': audio_file_path,
                'file_size': os.path.getsize(audio_file_path),
                'processed_at': datetime.now().isoformat(),
                'total_segments': len(segments),
                'total_duration': sum(seg['duration'] for seg in segments),
                'speakers': speaker_groups,
                'raw_transcriptions': transcriptions,
                'segmentation_method': segmentation_method,
                'processing_info': {
                    'echo_removed': self.config.get('REMOVE_ECHO', True),
                    'audio_normalized': self.config.get('AUDIO_NORMALIZE', True),
                    'advanced_segmentation': True,
                    'segmentation_method': segmentation_method,
                    'whisper_model': self.config.get('WHISPER_MODEL', 'whisper-1'),
                    'original_audio_file': audio_file_path,
                    'prepared_audio_file': prepared_audio_path
                }
            }
            
            self.logger.info(f"–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç: {len(speaker_groups)} —Å–ø–∏–∫–µ—Ä–æ–≤")
            return result
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {e}")
            # Fallback –Ω–∞ –æ–±—ã—á–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
            return self.process_audio_file_full(audio_file_path)
        
    def _prepare_audio(self, audio_file_path: str) -> str:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞—É–¥–∏–æ: –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —ç—Ö–∞, —Å–∂–∞—Ç–∏–µ –¥–ª—è API"""
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ
            temp_dir = self.config.get('TEMP_AUDIO_ROOT', 'data/temp_audio')
            os.makedirs(temp_dir, exist_ok=True)
            
            base_name = os.path.splitext(os.path.basename(audio_file_path))[0]
            prepared_audio_path = os.path.join(temp_dir, f"{base_name}_prepared.wav")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            file_ext = os.path.splitext(audio_file_path)[1].lower()
            
            # –ö–æ–º–∞–Ω–¥–∞ FFmpeg –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∞—É–¥–∏–æ —Å —Å–∂–∞—Ç–∏–µ–º
            ffmpeg_cmd = [
                'ffmpeg', '-i', audio_file_path,
                '-vn',  # —É–±–∏—Ä–∞–µ–º –≤–∏–¥–µ–æ
                '-ac', '1',  # –º–æ–Ω–æ
                '-ar', '16000',  # 16 –∫–ì—Ü
                '-b:a', '64k',  # –±–∏—Ç—Ä–µ–π—Ç 64 –∫–±–∏—Ç/—Å –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞
            ]
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–∫
            filters = []
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–æ–º–∫–æ—Å—Ç–∏
            if self.config.get('AUDIO_NORMALIZE', True):
                filters.append('loudnorm=I=-16:TP=-1.5:LRA=11')
            
            # –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —ç—Ö–∞
            if self.config.get('REMOVE_ECHO', True):
                echo_method = self.config.get('ECHO_REMOVAL_METHOD', 'ffmpeg')
                if echo_method == 'ffmpeg':
                    filters.extend([
                        'highpass=f=200',  # —É–±–∏—Ä–∞–µ–º –Ω–∏–∑–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã
                        'lowpass=f=8000',   # —É–±–∏—Ä–∞–µ–º –≤—ã—Å–æ–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã
                        'anlmdn',           # –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–µ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ
                        'anlms'             # –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ
                    ])
                elif echo_method == 'aggressive':
                    filters.extend([
                        'highpass=f=150',
                        'lowpass=f=4000',
                        'anlmdn',
                        'anlms',
                        'compand=0.2|0.2:1|1:-90/-60/-40/-20/-10/0:8:0:-90:0.1'
                    ])
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
            if filters:
                ffmpeg_cmd.extend(['-af', ','.join(filters)])
            
            # –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª
            ffmpeg_cmd.extend(['-y', prepared_audio_path])
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∫–æ–º–∞–Ω–¥—É
            self.logger.info(f"–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—é –∞—É–¥–∏–æ: {' '.join(ffmpeg_cmd)}")
            result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)
            
            if result.returncode != 0:
                self.logger.error(f"FFmpeg –æ—à–∏–±–∫–∞: {result.stderr}")
                # –ü—Ä–æ–±—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –∫–æ–º–∞–Ω–¥—É –±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤, –Ω–æ —Å —Å–∂–∞—Ç–∏–µ–º
                simple_cmd = [
                    'ffmpeg', '-i', audio_file_path,
                    '-vn', '-ac', '1', '-ar', '16000', '-b:a', '64k',
                    '-y', prepared_audio_path
                ]
                self.logger.info(f"–ü—Ä–æ–±—É—é —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –∫–æ–º–∞–Ω–¥—É: {' '.join(simple_cmd)}")
                result = subprocess.run(simple_cmd, capture_output=True, text=True)
                
                if result.returncode != 0:
                    self.logger.error(f"–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞ —Ç–æ–∂–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞: {result.stderr}")
                    return audio_file_path
            else:
                self.logger.warning(f"FFmpeg –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: {result.stderr}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            file_size = os.path.getsize(prepared_audio_path)
            self.logger.info(f"–†–∞–∑–º–µ—Ä –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {file_size / (1024*1024):.2f} MB")
            
            # –ï—Å–ª–∏ —Ñ–∞–π–ª –≤—Å–µ –µ—â–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–∂–∏–º–∞–µ–º
            if file_size > 20 * 1024 * 1024:  # –ë–æ–ª—å—à–µ 20 MB
                self.logger.info("–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–∂–∏–º–∞—é...")
                compressed_path = prepared_audio_path.replace('.wav', '_compressed.wav')
                compress_cmd = [
                    'ffmpeg', '-i', prepared_audio_path,
                    '-vn', '-ac', '1', '-ar', '8000', '-b:a', '32k',  # –ï—â–µ –±–æ–ª—å—à–µ —Å–∂–∏–º–∞–µ–º
                    '-y', compressed_path
                ]
                result = subprocess.run(compress_cmd, capture_output=True, text=True)
                if result.returncode == 0:
                    prepared_audio_path = compressed_path
                    file_size = os.path.getsize(prepared_audio_path)
                    self.logger.info(f"–†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ —Å–∂–∞—Ç–∏—è: {file_size / (1024*1024):.2f} MB")
            
            self.logger.info(f"–ê—É–¥–∏–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ: {prepared_audio_path}")
            return prepared_audio_path
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∞—É–¥–∏–æ: {e}")
            return audio_file_path
            
    def _remove_echo_python(self, audio_path: str):
        """–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —ç—Ö–∞ —á–µ—Ä–µ–∑ Python"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
            y, sr = librosa.load(audio_path, sr=16000)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ
            y_clean = nr.reduce_noise(
                y=y, sr=sr,
                stationary=False,
                prop_decrease=self.config.get('NOISE_REDUCE_PROP_DECREASE', 0.9),
                n_std_thresh_stationary=self.config.get('NOISE_REDUCE_STD_THRESH', 2.0)
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ
            librosa.output.write_wav(audio_path, y_clean, sr)
            
        except Exception as e:
            self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å Python —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ: {e}")
            
    def _split_audio_by_voice_characteristics(self, audio_file_path: str) -> List[Dict[str, Any]]:
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∞—É–¥–∏–æ –ø–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º –≥–æ–ª–æ—Å–∞ (—Ç–æ–Ω, —Ç–µ–º–±—Ä, MFCC)"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
            y, sr = librosa.load(audio_file_path, sr=16000)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞
            method = self.config.get('VOICE_ANALYSIS_METHOD', 'mfcc')
            
            if method == 'mfcc':
                change_points = self._detect_speaker_changes_mfcc(y, sr)
            elif method == 'spectral':
                change_points = self._detect_speaker_changes_spectral(y, sr)
            elif method == 'combined':
                change_points = self._detect_speaker_changes_combined(y, sr)
            else:
                # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥
                return self._split_audio_by_silence_fallback(audio_file_path)
            
            # –°–æ–∑–¥–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã
            segments = self._create_segments_from_changes(y, sr, change_points)
            
            # –ï—Å–ª–∏ —Å–µ–≥–º–µ–Ω—Ç—ã —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ, –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –∏—Ö
            min_duration = self.config.get('MIN_SEGMENT_DURATION', 5000)  # 5 —Å–µ–∫—É–Ω–¥
            segments = self._merge_short_segments(segments, min_duration)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            self._save_segments_for_analysis(segments, audio_file_path)
            
            self.logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –ø–æ –∞–Ω–∞–ª–∏–∑—É –≥–æ–ª–æ—Å–∞")
            return segments
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –≥–æ–ª–æ—Å–∞: {e}")
            return self._split_audio_by_silence_fallback(audio_file_path)
            
    def _save_segments_for_analysis(self, segments: List[Dict[str, Any]], original_audio_path: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            base_name = os.path.splitext(os.path.basename(original_audio_path))[0]
            segments_dir = os.path.join(
                self.config.get('AUDIO_PROCESSING_ROOT', 'data/audio_processing'),
                f"{base_name}_segments"
            )
            os.makedirs(segments_dir, exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–π —Å–µ–≥–º–µ–Ω—Ç
            for i, segment in enumerate(segments):
                segment_filename = f"segment_{i+1:03d}_{segment['start_time']:06d}ms_{segment['end_time']:06d}ms.wav"
                segment_path = os.path.join(segments_dir, segment_filename)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—É–¥–∏–æ —Å–µ–≥–º–µ–Ω—Ç–∞
                import soundfile as sf
                sf.write(segment_path, segment['audio'], 16000)
                
                # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç–∞
                metadata = {
                    'segment_number': i + 1,
                    'start_time_ms': segment['start_time'],
                    'end_time_ms': segment['end_time'],
                    'duration_ms': segment['duration'],
                    'start_time_s': segment['start_time'] / 1000,
                    'end_time_s': segment['end_time'] / 1000,
                    'duration_s': segment['duration'] / 1000,
                    'audio_file': segment_filename
                }
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                metadata_path = segment_path.replace('.wav', '.json')
                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—â–∏–π —Ñ–∞–π–ª —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ö
            segments_info = {
                'original_file': original_audio_path,
                'total_segments': len(segments),
                'total_duration_ms': sum(seg['duration'] for seg in segments),
                'total_duration_s': sum(seg['duration'] for seg in segments) / 1000,
                'segments': []
            }
            
            for i, segment in enumerate(segments):
                segments_info['segments'].append({
                    'segment_number': i + 1,
                    'start_time_ms': segment['start_time'],
                    'end_time_ms': segment['end_time'],
                    'duration_ms': segment['duration'],
                    'start_time_s': segment['start_time'] / 1000,
                    'end_time_s': segment['end_time'] / 1000,
                    'duration_s': segment['duration'] / 1000,
                    'audio_file': f"segment_{i+1:03d}_{segment['start_time']:06d}ms_{segment['end_time']:06d}ms.wav",
                    'metadata_file': f"segment_{i+1:03d}_{segment['start_time']:06d}ms_{segment['end_time']:06d}ms.json"
                })
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            info_path = os.path.join(segments_dir, 'segments_info.json')
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(segments_info, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"–°–µ–≥–º–µ–Ω—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤: {segments_dir}")
            
        except Exception as e:
            self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–µ–≥–º–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {e}")
            
    def _detect_speaker_changes_mfcc(self, y: np.ndarray, sr: int) -> List[int]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–º–µ–Ω—ã –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –ø–æ MFCC —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        try:
            # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã MFCC
            hop_length = int(self.config.get('MFCC_HOP_SIZE', 5) * sr / 1000)  # 5–º—Å –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            n_fft = int(self.config.get('MFCC_WINDOW_SIZE', 20) * sr / 1000)   # 20–º—Å –æ–∫–Ω–æ
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º MFCC —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤
            mfcc = librosa.feature.mfcc(
                y=y, sr=sr, n_mfcc=20,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤
                hop_length=hop_length, n_fft=n_fft
            )
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º MFCC
            mfcc_normalized = (mfcc - np.mean(mfcc, axis=1, keepdims=True)) / (np.std(mfcc, axis=1, keepdims=True) + 1e-8)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ —Ñ—Ä–µ–π–º–∞–º–∏
            distances = []
            for i in range(1, mfcc_normalized.shape[1]):
                dist = np.linalg.norm(mfcc_normalized[:, i] - mfcc_normalized[:, i-1])
                distances.append(dist)
            
            # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
            threshold = np.mean(distances) + self.config.get('VOICE_CHANGE_THRESHOLD', 1.2) * np.std(distances)
            
            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏ (2 —Å–µ–∫—É–Ω–¥—ã)
            min_distance = int(2.0 * sr / hop_length)
            
            # –ù–∞—Ö–æ–¥–∏–º –ø–∏–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è
            change_points = find_peaks(
                distances, 
                height=threshold, 
                distance=min_distance,
                prominence=np.std(distances) * 0.5  # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –ø–∏–∫–∞
            )[0]
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            change_times = [int(point * hop_length / sr * 1000) for point in change_points]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
            if not change_times or change_times[0] > 5000:  # –ï—Å–ª–∏ –ø–µ—Ä–≤–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ 5 —Å–µ–∫—É–Ω–¥
                change_times.insert(0, 0)
            
            total_duration = len(y) / sr * 1000
            if not change_times or change_times[-1] < total_duration - 5000:  # –ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∑–∞ 5 —Å–µ–∫—É–Ω–¥ –¥–æ –∫–æ–Ω—Ü–∞
                change_times.append(int(total_duration))
            
            return change_times
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ MFCC –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return []
            
    def _detect_speaker_changes_spectral(self, y: np.ndarray, sr: int) -> List[int]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–º–µ–Ω—ã –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –ø–æ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏"""
        try:
            # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            hop_length = int(5 * sr / 1000)  # 5–º—Å –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–æ–ª—å—à–µ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=hop_length)[0]
            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, hop_length=hop_length)[0]
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr, hop_length=hop_length)[0]
            spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr, hop_length=hop_length)[0]
            
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = np.vstack([
                spectral_centroid, spectral_rolloff, spectral_bandwidth,
                np.mean(spectral_contrast, axis=0)  # –°—Ä–µ–¥–Ω–µ–µ –ø–æ –∫–æ–Ω—Ç—Ä–∞—Å—Ç—É
            ])
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features_normalized = (features - np.mean(features, axis=1, keepdims=True)) / (np.std(features, axis=1, keepdims=True) + 1e-8)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å –≤–µ—Å–∞–º–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            weights = [1.0, 0.8, 0.6, 0.4]  # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            changes = np.zeros(features_normalized.shape[1] - 1)
            
            for i in range(features_normalized.shape[1] - 1):
                for j, weight in enumerate(weights):
                    changes[i] += weight * (features_normalized[j, i+1] - features_normalized[j, i]) ** 2
            
            # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥
            threshold = np.mean(changes) + self.config.get('VOICE_CHANGE_THRESHOLD', 1.2) * np.std(changes)
            min_distance = int(2.0 * sr / hop_length)  # 2 —Å–µ–∫—É–Ω–¥—ã
            
            # –ù–∞—Ö–æ–¥–∏–º –ø–∏–∫–∏
            change_points = find_peaks(
                changes, 
                height=threshold, 
                distance=min_distance,
                prominence=np.std(changes) * 0.5
            )[0]
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã
            change_times = [int(point * hop_length / sr * 1000) for point in change_points]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü
            if not change_times or change_times[0] > 5000:
                change_times.insert(0, 0)
            
            total_duration = len(y) / sr * 1000
            if not change_times or change_times[-1] < total_duration - 5000:
                change_times.append(int(total_duration))
            
            return change_times
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return []
            
    def _detect_speaker_changes_combined(self, y: np.ndarray, sr: int) -> List[int]:
        """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–º–µ–Ω—ã –≥–æ–≤–æ—Ä—è—â–µ–≥–æ"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤
            mfcc_changes = self._detect_speaker_changes_mfcc(y, sr)
            spectral_changes = self._detect_speaker_changes_spectral(y, sr)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            all_changes = mfcc_changes + spectral_changes
            all_changes.sort()
            
            # –£–±–∏—Ä–∞–µ–º –±–ª–∏–∑–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 500–º—Å)
            filtered_changes = []
            for change in all_changes:
                if not filtered_changes or change - filtered_changes[-1] > 500:
                    filtered_changes.append(change)
            
            return filtered_changes
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return []
            
    def _create_segments_from_changes(self, y: np.ndarray, sr: int, change_points: List[int]) -> List[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ç–æ—á–µ–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
        segments = []
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ—á–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        change_points = sorted(change_points)
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –º–µ–∂–¥—É —Ç–æ—á–∫–∞–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        for i in range(len(change_points)):
            start_time = change_points[i]
            
            if i + 1 < len(change_points):
                end_time = change_points[i + 1]
            else:
                end_time = int(len(y) / sr * 1000)
            
            duration = end_time - start_time
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞
            min_duration = self.config.get('MIN_SEGMENT_DURATION', 5000)  # 5 —Å–µ–∫—É–Ω–¥
            
            if duration >= min_duration:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—É–¥–∏–æ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞
                start_sample = int(start_time / 1000 * sr)
                end_sample = int(end_time / 1000 * sr)
                
                if start_sample < len(y) and end_sample <= len(y):
                    segment_audio = y[start_sample:end_sample]
                    
                    segments.append({
                        'start_time': start_time,
                        'end_time': end_time,
                        'duration': duration,
                        'audio': segment_audio,
                        'start_sample': start_sample,
                        'end_sample': end_sample
                    })
                else:
                    self.logger.warning(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã —Å–µ–≥–º–µ–Ω—Ç–∞: {start_sample}-{end_sample}, –¥–ª–∏–Ω–∞ –∞—É–¥–∏–æ: {len(y)}")
            else:
                self.logger.info(f"–°–µ–≥–º–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π: {duration}–º—Å, –ø—Ä–æ–ø—É—Å–∫–∞—é")
        
        return segments
        
    def _merge_short_segments(self, segments: List[Dict[str, Any]], min_duration: int) -> List[Dict[str, Any]]:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
        if not segments:
            return segments
            
        merged = []
        current_segment = segments[0].copy()
        
        for segment in segments[1:]:
            # –ï—Å–ª–∏ —Ç–µ–∫—É—â–∏–π —Å–µ–≥–º–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –æ–±—ä–µ–¥–∏–Ω—è–µ–º
            if current_segment['duration'] < min_duration:
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —Ç–µ–∫—É—â–∏–º —Å–µ–≥–º–µ–Ω—Ç–æ–º
                current_segment['end_time'] = segment['end_time']
                current_segment['duration'] = segment['end_time'] - current_segment['start_time']
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∞—É–¥–∏–æ
                if 'audio' in current_segment and 'audio' in segment:
                    current_segment['audio'] = np.concatenate([current_segment['audio'], segment['audio']])
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã —Å—ç–º–ø–ª–æ–≤
                if 'end_sample' in segment:
                    current_segment['end_sample'] = segment['end_sample']
                
                self.logger.info(f"–û–±—ä–µ–¥–∏–Ω–∏–ª –∫–æ—Ä–æ—Ç–∫–∏–π —Å–µ–≥–º–µ–Ω—Ç: {current_segment['start_time']}–º—Å - {current_segment['end_time']}–º—Å")
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π —Å–µ–≥–º–µ–Ω—Ç –∏ –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π
                merged.append(current_segment)
                current_segment = segment.copy()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
        merged.append(current_segment)
        
        self.logger.info(f"–ü–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è: {len(merged)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
        return merged
        
    def _split_audio_by_silence_fallback(self, audio_file_path: str) -> List[Dict[str, Any]]:
        """Fallback: —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏—à–∏–Ω–µ (—Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥)"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ —Ñ–∞–π–ª
            audio = AudioSegment.from_file(audio_file_path)
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø–æ —Ç–∏—à–∏–Ω–µ
            min_silence_len = self.config.get('SPEAKER_MIN_DURATION', 3000)  # 3 —Å–µ–∫—É–Ω–¥—ã
            silence_thresh = self.config.get('SPEAKER_SILENCE_THRESHOLD', -35)  # -35 dB
            
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–º–µ—é—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ç–∏–ø
            try:
                if isinstance(min_silence_len, str):
                    min_silence_len = float(min_silence_len) * 1000
                min_silence_len = int(min_silence_len)
                
                if isinstance(silence_thresh, str):
                    silence_thresh = float(silence_thresh)
                silence_thresh = int(silence_thresh)
            except (ValueError, TypeError) as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {e}")
                min_silence_len = 3000  # 3 —Å–µ–∫—É–Ω–¥—ã
                silence_thresh = -35     # -35 dB
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ —Ç–∏—à–∏–Ω–µ
            chunks = split_on_silence(
                audio,
                min_silence_len=min_silence_len,
                silence_thresh=silence_thresh,
                keep_silence=200  # –æ—Å—Ç–∞–≤–ª—è–µ–º –Ω–µ–º–Ω–æ–≥–æ —Ç–∏—à–∏–Ω—ã
            )
            
            # –ï—Å–ª–∏ —Å–µ–≥–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            if not chunks:
                self.logger.info("–°–µ–≥–º–µ–Ω—Ç—ã –ø–æ —Ç–∏—à–∏–Ω–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback - —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏")
                segment_duration = self.config.get('FALLBACK_SEGMENT_DURATION', 30000)  # 30 —Å–µ–∫—É–Ω–¥
                chunks = []
                for i in range(0, len(audio), segment_duration):
                    chunk = audio[i:i + segment_duration]
                    if len(chunk) > 0:
                        chunks.append(chunk)
            
            # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ö
            segments = []
            current_time = 0
            
            for i, chunk in enumerate(chunks):
                duration = len(chunk)
                segment_info = {
                    'start_time': current_time,
                    'end_time': current_time + duration,
                    'duration': duration,
                    'audio': chunk
                }
                segments.append(segment_info)
                current_time += duration
                
            return segments
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ –∞—É–¥–∏–æ: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–¥–∏–Ω —Å–µ–≥–º–µ–Ω—Ç —Å –ø–æ–ª–Ω—ã–º —Ñ–∞–π–ª–æ–º
            audio = AudioSegment.from_file(audio_file_path)
            return [{
                'start_time': 0,
                'end_time': len(audio),
                'duration': len(audio),
                'audio': audio
            }]

    def _save_segment_temp(self, segment: Dict[str, Any], segment_index: int) -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–∞ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª"""
        temp_dir = self.config.get('TEMP_AUDIO_ROOT', 'temp/audio')
        os.makedirs(temp_dir, exist_ok=True)
        
        temp_file = os.path.join(temp_dir, f"segment_{segment_index}.wav")
        
        # –ï—Å–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å numpy –º–∞—Å—Å–∏–≤, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –µ–≥–æ
        if 'audio' in segment and isinstance(segment['audio'], np.ndarray):
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º numpy –º–∞—Å—Å–∏–≤ –∫–∞–∫ WAV
            import soundfile as sf
            sf.write(temp_file, segment['audio'], 16000)
        else:
            # Fallback –Ω–∞ pydub
            segment['audio'].export(temp_file, format="wav")
            
        return temp_file
        
    def _transcribe_with_whisper(self, segment: Dict[str, Any], prepared_audio_path: str) -> Optional[str]:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥"""
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞
            temp_file = self._save_segment_temp(segment, segment['start_time'])
            
            if self.transcription_method == 'openai':
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º OpenAI API
                return self._transcribe_segment_with_openai_api(temp_file)
            elif self.transcription_method == 'whisper':
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π Whisper
                return self._transcribe_segment_with_local_whisper(temp_file)
            else:
                self.logger.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {self.transcription_method}")
                return None
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Å–µ–≥–º–µ–Ω—Ç–∞: {e}")
            return None
            
    def _transcribe_segment_with_openai_api(self, temp_file: str) -> Optional[str]:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ OpenAI API"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            model = self.config.get('WHISPER_MODEL', 'whisper-1')
            language = self.config.get('WHISPER_LANGUAGE', 'ru')
            
            with open(temp_file, "rb") as audio_file:
                # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è API
                params = {
                    'model': model,
                    'file': audio_file,
                    'response_format': 'verbose_json'
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º —è–∑—ã–∫ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω –Ω–µ None
                if language and language != 'auto':
                    params['language'] = language
                
                response = self.client.audio.transcriptions.create(**params)
                
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            os.remove(temp_file)
            
            return response.text.strip()
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ OpenAI API —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Å–µ–≥–º–µ–Ω—Ç–∞: {e}")
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            if os.path.exists(temp_file):
                os.remove(temp_file)
            return None
            
    def _transcribe_segment_with_local_whisper(self, temp_file: str) -> Optional[str]:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—ã–π Whisper"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            language = self.config.get('WHISPER_LANGUAGE', 'ru')
            task = self.config.get('WHISPER_TASK', 'transcribe')
            
            # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º —Å–µ–≥–º–µ–Ω—Ç
            result = self.whisper_model.transcribe(
                temp_file,
                language=language,
                task=task
            )
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            os.remove(temp_file)
            
            return result['text'].strip()
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π Whisper —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Å–µ–≥–º–µ–Ω—Ç–∞: {e}")
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            if os.path.exists(temp_file):
                os.remove(temp_file)
            return None
            
    def _transcribe_full_audio(self, audio_file_path: str) -> Optional[str]:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        if self.transcription_method == 'whisper':
            return self._transcribe_with_openai_whisper_local(audio_file_path)
        else:
            return self._transcribe_with_openai_api(audio_file_path)
            
    def _transcribe_with_openai_api(self, audio_file_path: str) -> Optional[str]:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI API —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                self.logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ OpenAI —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ {attempt + 1}/{max_retries}")
                
                # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Whisper
                with open(audio_file_path, 'rb') as audio_file:
                    response = self.client.audio.transcriptions.create(
                        model=self.config.get('WHISPER_MODEL', 'whisper-1'),
                        file=audio_file,
                        response_format='verbose_json',
                        language='ru'  # –£–∫–∞–∑—ã–≤–∞–µ–º —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
                    )
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
                    if hasattr(response, 'text'):
                        self.logger.info("OpenAI —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                        return response.text
                    elif hasattr(response, 'content'):
                        self.logger.info("OpenAI —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                        return response.content
                    else:
                        self.logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ OpenAI: {type(response)}")
                        return None
                        
            except Exception as e:
                error_msg = str(e)
                self.logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ —É–¥–∞–ª–∞—Å—å: {error_msg}")
                
                if attempt < max_retries - 1:
                    self.logger.info(f"–û–∂–∏–¥–∞—é {retry_delay} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                    time.sleep(retry_delay)
                    retry_delay *= 2  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
                else:
                    self.logger.error(f"–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ OpenAI —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã: {error_msg}")
                    return None
        
        return None
            
    def _transcribe_with_openai_whisper_local(self, audio_file_path: str) -> Optional[str]:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—ã–π OpenAI Whisper"""
        try:
            self.logger.info("–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é —á–µ—Ä–µ–∑ OpenAI Whisper (–ª–æ–∫–∞–ª—å–Ω–æ)...")
            
            # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º –∞—É–¥–∏–æ
            result = self.whisper_model.transcribe(
                audio_file_path,
                language='ru',
                task='transcribe'
            )
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            full_text = result['text'].strip()
            
            self.logger.info(f"OpenAI Whisper (–ª–æ–∫–∞–ª—å–Ω–æ) —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –î–ª–∏–Ω–∞: {len(full_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            return full_text
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ OpenAI Whisper (–ª–æ–∫–∞–ª—å–Ω–æ) —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {e}")
            return None
            
    def _get_audio_duration(self, audio_file_path: str) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞ –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º librosa –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            y, sr = librosa.load(audio_file_path, sr=None)
            duration_ms = int(len(y) / sr * 1000)
            return duration_ms
        except Exception as e:
            self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {e}")
            return 0
            
    def _group_by_speakers_improved(self, transcriptions: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º"""
        if not transcriptions:
            self.logger.warning("–ù–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏")
            return {}
            
        # –£–ª—É—á—à–µ–Ω–Ω–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –ø–∞—É–∑–∞–º
        speakers = {}
        current_speaker = 0
        max_speakers = self.config.get('MAX_SPEAKERS', 10)
        
        for i, trans in enumerate(transcriptions):
            # –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤—ã–π —Å–µ–≥–º–µ–Ω—Ç –∏–ª–∏ –ø—Ä–æ—à–ª–æ –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏, –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
            pause_threshold = self.config.get('SPEAKER_PAUSE_THRESHOLD', 5000)  # 5 —Å–µ–∫—É–Ω–¥
            
            if i == 0 or (i > 0 and trans['start_time'] - transcriptions[i-1]['end_time'] > pause_threshold):
                current_speaker = (current_speaker + 1) % max_speakers
            
            speaker_name = f"–£—á–∞—Å—Ç–Ω–∏–∫ {current_speaker + 1}"
            if speaker_name not in speakers:
                speakers[speaker_name] = []
                
            speakers[speaker_name].append({
                'text': trans['text'],
                'start_time': trans['start_time'],
                'end_time': trans['end_time'],
                'duration': trans['duration']
            })
            
        self.logger.info(f"–°–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–æ –≤ {len(speakers)} —Å–ø–∏–∫–µ—Ä–æ–≤")
        return speakers
        
    def _save_result(self, result: Dict[str, Any], output_format: str = 'json'):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ —Ñ–∞–π–ª"""
        output_dir = self.config.get('TRANSCRIPT_OUTPUT_ROOT', 'data/transcripts')
        os.makedirs(output_dir, exist_ok=True)
        
        base_name = os.path.splitext(os.path.basename(result['file_path']))[0]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if output_format == 'json':
            output_file = os.path.join(output_dir, f"{base_name}_{timestamp}.json")
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
                
        elif output_format == 'txt':
            output_file = os.path.join(output_dir, f"{base_name}_{timestamp}.txt")
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è: {result['file_path']}\n")
                f.write(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {result['processed_at']}\n")
                f.write(f"–í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {result['total_segments']}\n")
                f.write(f"–û–±—â–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {result['total_duration']} –º—Å\n\n")
                
                for speaker, segments in result['speakers'].items():
                    f.write(f"=== {speaker} ===\n")
                    for seg in segments:
                        f.write(f"[{self._ms_to_srt_time(seg['start_time'])} - {self._ms_to_srt_time(seg['end_time'])}]\n")
                        f.write(f"{seg['text']}\n\n")
                        
        elif output_format == 'srt':
            output_file = os.path.join(output_dir, f"{base_name}_{timestamp}.srt")
            with open(output_file, 'w', encoding='utf-8') as f:
                subtitle_index = 1
                for speaker, segments in result['speakers'].items():
                    for seg in segments:
                        f.write(f"{subtitle_index}\n")
                        f.write(f"{self._ms_to_srt_time(seg['start_time'])} --> {self._ms_to_srt_time(seg['end_time'])}\n")
                        f.write(f"{speaker}: {seg['text']}\n\n")
                        subtitle_index += 1
                        
        self.logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_file}")
        
    def _ms_to_srt_time(self, ms: int) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥ –≤ —Ñ–æ—Ä–º–∞—Ç –≤—Ä–µ–º–µ–Ω–∏ SRT"""
        seconds = ms // 1000
        milliseconds = ms % 1000
        hours = seconds // 3600
        minutes = (seconds % 3600) // 60
        seconds = seconds % 60
        
        return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"
        
    def cleanup_temp_files(self):
        """–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        temp_dir = self.config.get('TEMP_AUDIO_ROOT', 'temp/audio')
        if os.path.exists(temp_dir):
            try:
                for file in os.listdir(temp_dir):
                    file_path = os.path.join(temp_dir, file)
                    if os.path.isfile(file_path):
                        os.remove(file_path)
                self.logger.info("–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –æ—á–∏—â–µ–Ω—ã")
            except Exception as e:
                self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã: {e}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    import argparse
    
    parser = argparse.ArgumentParser(description='–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª–æ–≤ —Å Whisper')
    parser.add_argument('audio_file', help='–ü—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É')
    parser.add_argument('--format', choices=['json', 'txt', 'srt'], default='json',
                       help='–§–æ—Ä–º–∞—Ç –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞')
    parser.add_argument('--config', help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏')
    
    args = parser.parse_args()
    
    try:
        processor = AudioProcessor(args.config)
        result = processor.process_audio_file(args.audio_file, args.format)
        processor._save_result(result, args.format)
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
