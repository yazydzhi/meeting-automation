#!/usr/bin/env python3
"""
–ü–æ–∏—Å–∫ Google —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–∏—Å—Ç–µ–º–µ
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –º–µ—Å—Ç–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤
"""

import os
import json
import glob
from pathlib import Path
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

def search_tokens_in_directory(base_path, pattern="*token*"):
    """–ò—â–µ—Ç —Ç–æ–∫–µ–Ω—ã –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
    tokens = []
    
    if not os.path.exists(base_path):
        return tokens
    
    try:
        # –ò—â–µ–º —Ñ–∞–π–ª—ã —Å —Ç–æ–∫–µ–Ω–∞–º–∏
        search_pattern = os.path.join(base_path, "**", pattern)
        for file_path in glob.glob(search_pattern, recursive=True):
            if os.path.isfile(file_path):
                tokens.append(file_path)
    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ {base_path}: {e}")
    
    return tokens

def analyze_token_file(file_path):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª —Ç–æ–∫–µ–Ω–∞"""
    print(f"\nüîç –ê–Ω–∞–ª–∏–∑: {file_path}")
    
    try:
        # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–∫ JSON
        with open(file_path, 'r') as f:
            content = f.read().strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —ç—Ç–æ JSON –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ–π —Ç–æ–∫–µ–Ω
        if content.startswith('{'):
            try:
                token_data = json.loads(content)
                print(f"  ‚úÖ JSON —Ç–æ–∫–µ–Ω –æ–±–Ω–∞—Ä—É–∂–µ–Ω")
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                if 'scopes' in token_data:
                    scopes = token_data['scopes']
                    if isinstance(scopes, str):
                        scopes = scopes.split(' ')
                    
                    print(f"  üìã Scopes ({len(scopes)}):")
                    for scope in scopes:
                        if scope.strip():
                            scope_name = scope.split('/')[-1]
                            print(f"    - {scope_name}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã API
                    calendar_scopes = [s for s in scopes if 'calendar' in s]
                    drive_scopes = [s for s in scopes if 'drive' in s]
                    
                    print(f"  üéØ API —Ç–∏–ø—ã:")
                    print(f"    üìÖ Calendar: {len(calendar_scopes)} scopes")
                    print(f"    üìÅ Drive: {len(drive_scopes)} scopes")
                    
                    if 'client_id' in token_data:
                        client_id = token_data['client_id']
                        project_id = client_id.split('-')[0] if '-' in client_id else 'N/A'
                        print(f"  üèóÔ∏è Project ID: {project_id}")
                        print(f"  üîë Client ID: {client_id}")
                    
                    return {
                        'type': 'json',
                        'scopes': scopes,
                        'calendar_scopes': len(calendar_scopes),
                        'drive_scopes': len(drive_scopes),
                        'client_id': token_data.get('client_id'),
                        'project_id': project_id if 'client_id' in token_data else None
                    }
                else:
                    print(f"  ‚ö†Ô∏è JSON –±–µ–∑ scopes")
                    return {'type': 'json_no_scopes'}
                    
            except json.JSONDecodeError:
                print(f"  ‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON")
                return {'type': 'invalid_json'}
        else:
            # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ç–æ–∫–µ–Ω
            token_length = len(content)
            print(f"  üìù –ü—Ä–æ—Å—Ç–æ–π —Ç–æ–∫–µ–Ω (–¥–ª–∏–Ω–∞: {token_length})")
            return {'type': 'simple_text', 'length': token_length}
            
    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è: {e}")
        return {'type': 'error', 'error': str(e)}

def check_environment_tokens():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–æ–∫–µ–Ω—ã –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
    print("üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–∫–µ–Ω—ã –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è...")
    
    env_tokens = []
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ .env
    personal_token = os.getenv('PERSONAL_GOOGLE_TOKEN')
    work_token = os.getenv('WORK_GOOGLE_TOKEN')
    
    if personal_token:
        env_tokens.append(('PERSONAL_GOOGLE_TOKEN', personal_token))
    
    if work_token:
        env_tokens.append(('WORK_GOOGLE_TOKEN', work_token))
    
    if env_tokens:
        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: {len(env_tokens)}")
        for var_name, var_path in env_tokens:
            if os.path.exists(var_path):
                print(f"    ‚úÖ {var_name}: {var_path}")
                analyze_token_file(var_path)
            else:
                print(f"    ‚ùå {var_name}: {var_path} (—Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω)")
    else:
        print("  ‚ùå –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    return env_tokens

def search_common_locations():
    """–ò—â–µ—Ç —Ç–æ–∫–µ–Ω—ã –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö"""
    print("\nüîç –ü–æ–∏—Å–∫ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö...")
    
    common_locations = [
        "~/Library/Application Support/Google",
        "~/.config/google",
        "~/.google",
        "~/.credentials",
        "~/Library/Preferences",
        "~/Library/Application Support/Google/DriveFS"
    ]
    
    all_tokens = []
    
    for location in common_locations:
        expanded_path = os.path.expanduser(location)
        print(f"\nüìÅ –ü—Ä–æ–≤–µ—Ä—è–µ–º: {expanded_path}")
        
        if os.path.exists(expanded_path):
            tokens = search_tokens_in_directory(expanded_path)
            if tokens:
                print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(tokens)}")
                for token_path in tokens:
                    all_tokens.append(token_path)
                    analyze_token_file(token_path)
            else:
                print(f"  ‚ö†Ô∏è –¢–æ–∫–µ–Ω—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        else:
            print(f"  ‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
    
    return all_tokens

def search_project_tokens():
    """–ò—â–µ—Ç —Ç–æ–∫–µ–Ω—ã –≤ —Ç–µ–∫—É—â–µ–º –ø—Ä–æ–µ–∫—Ç–µ"""
    print("\nüîç –ü–æ–∏—Å–∫ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—É—â–µ–º –ø—Ä–æ–µ–∫—Ç–µ...")
    
    project_locations = [
        "./tokens",
        "./creds", 
        "./credentials",
        "./.tokens",
        "./.credentials"
    ]
    
    project_tokens = []
    
    for location in project_locations:
        if os.path.exists(location):
            print(f"\nüìÅ –ü—Ä–æ–≤–µ—Ä—è–µ–º: {location}")
            tokens = search_tokens_in_directory(location)
            if tokens:
                print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(tokens)}")
                for token_path in tokens:
                    project_tokens.append(token_path)
                    analyze_token_file(token_path)
            else:
                print(f"  ‚ö†Ô∏è –¢–æ–∫–µ–Ω—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        else:
            print(f"  ‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {location}")
    
    return project_tokens

def generate_token_report(env_tokens, common_tokens, project_tokens):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö"""
    print("\n" + "=" * 80)
    print("üìä –û–¢–ß–ï–¢ –û –ü–û–ò–°–ö–ï GOOGLE –¢–û–ö–ï–ù–û–í")
    print("=" * 80)
    
    total_tokens = len(env_tokens) + len(common_tokens) + len(project_tokens)
    
    print(f"\nüìà –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"  üîß –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è: {len(env_tokens)}")
    print(f"  üè† –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Å—Ç–∞: {len(common_tokens)}")
    print(f"  üìÅ –ü—Ä–æ–µ–∫—Ç: {len(project_tokens)}")
    print(f"  üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {total_tokens}")
    
    if total_tokens == 0:
        print(f"\n‚ùå –¢–æ–∫–µ–Ω—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        print(f"üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print(f"  1. –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é Google API")
        print(f"  2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ OAuth")
        print(f"  3. –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ API –≤–∫–ª—é—á–µ–Ω—ã –≤ Cloud Console")
    else:
        print(f"\n‚úÖ –¢–æ–∫–µ–Ω—ã –Ω–∞–π–¥–µ–Ω—ã!")
        print(f"üí° –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
        print(f"  1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã")
        print(f"  2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞ (scopes)")
        print(f"  3. –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å —Ç–æ–∫–µ–Ω—ã")
    
    print(f"\n{'=' * 80}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤"""
    print("üîç –ü–û–ò–°–ö GOOGLE –¢–û–ö–ï–ù–û–í –í –°–ò–°–¢–ï–ú–ï")
    print("üéØ –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –º–µ—Å—Ç–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è")
    print("=" * 80)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–∫–µ–Ω—ã –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    env_tokens = check_environment_tokens()
    
    # –ò—â–µ–º –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
    common_tokens = search_common_locations()
    
    # –ò—â–µ–º –≤ –ø—Ä–æ–µ–∫—Ç–µ
    project_tokens = search_project_tokens()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    generate_token_report(env_tokens, common_tokens, project_tokens)
    
    print("\nüéä –ü–æ–∏—Å–∫ —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == '__main__':
    main()
